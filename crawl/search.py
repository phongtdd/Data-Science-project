import httpx
import random
from typing import List
from typing_extensions import TypedDict
from loguru import logger as log
from bs4 import BeautifulSoup
import json
import asyncio
import re
from urllib.parse import urljoin

SEARCH_QUERY = "swimsuit"
UNIQUE_PRODUCT_SAVE_FILE = f"data/{SEARCH_QUERY}/amazon_unique_{SEARCH_QUERY}.json"


class ProductPreview(TypedDict):
    """Result generated by search scraper."""
    url: str
    asin: str


def extract_asin(url: str) -> str:
    """Extract ASIN from the product URL."""
    match = re.search(r"/dp/([A-Z0-9]{10})", url)
    return match.group(1) if match else ""


async def parse_search(resp, semaphore) -> List[ProductPreview]:
    """Parse search result page for product URLs and ASINs using BeautifulSoup."""
    previews = []
    
    # Parse HTML with BeautifulSoup
    soup = BeautifulSoup(resp.text, "html.parser")

    # Find product links by looking for divs containing product information
    product_boxes = soup.find_all("div", {"data-component-type": "s-search-result"})
    
    for box in product_boxes:
        # Extract the product URL
        link_tag = box.find("h2").find("a")  # The <a> tag inside <h2> contains the link
        if link_tag:
            url = link_tag["href"]
            url = urljoin(str(resp.url), url).split("?")[0]  # Clean URL by removing query params

            # Skip non-product links that don't contain '/dp/'
            if "/dp/" not in url:
                continue

            # Extract ASIN from the URL
            asin = extract_asin(url)

            # If ASIN is found, add the product URL and ASIN to the list
            if asin:
                previews.append({"url": url, "asin": asin})

    log.debug(f"Found {len(previews)} product listings in {resp.url}")
    return previews


async def search(query, session, semaphore):
    """Search for Amazon products using search box and fetch product links and ASINs."""
    log.info(f"{query}: Scraping first page")

    # First, scrape the first query page to get product links
    first_page = await session.get(f"https://www.amazon.com/s?k={query}&page=1")

    total_pages = 100  # Set an estimated number of pages to scrape

    # Now we can scrape remaining pages concurrently with limited concurrent requests
    log.info(f"{query}: Found {total_pages} pages, scraping them concurrently.")
    other_pages = await asyncio.gather(
        *[session.get(f"https://www.amazon.com/s?k={query}&page={page}") for page in range(2, total_pages + 1)]
    )

    # Parse all of the search pages for product URLs and ASINs
    all_previews = []
    for response in [first_page, *other_pages]:
        page_previews = await parse_search(response, semaphore)  # Pass semaphore to parse_search
        all_previews.extend(page_previews)

    log.info(f"{query}: Found a total of {len(all_previews)} product links.")
    return all_previews


BASE_HEADERS = [
    {
        "accept-language": "en-US,en;q=0.8",
        "user-agent": "Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36",
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
        "accept-encoding": "gzip, deflate, br"
    },
    {
        "accept-language": "en-US,en;q=0.7",
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_16_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36",
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
        "accept-encoding": "gzip, deflate, br"
    },
    {
        "accept-language": "en-US,en;q=0.6",
        "user-agent": "Mozilla/5.0 (Linux; U; Android 11; en-US; Pixel 4 Build/RQ2A.210305.006) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30",
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
        "accept-encoding": "gzip, deflate, br"
    },
    {
        "accept-language": "en-US,en;q=0.9",
        "user-agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
        "accept-encoding": "gzip, deflate, br"
    }
]



async def run():
    """Run the scraping process."""
    limits = httpx.Limits(max_connections=8)  # Set max concurrent connections
    semaphore = asyncio.Semaphore(5)  # Set the limit for concurrent requests to 5

    async with httpx.AsyncClient(limits=limits, timeout=httpx.Timeout(15.0)) as session:
        # Rotate headers by picking a random one from the list
        headers = random.choice(BASE_HEADERS)

        # Pass the rotated header to each request
        session.headers.update(headers)

        # Perform the search and fetch product links and ASINs
        data = await search("XL One-piece" + SEARCH_QUERY, session=session, semaphore=semaphore)

        # Save product URLs and ASINs
        with open(UNIQUE_PRODUCT_SAVE_FILE, "w") as json_file:
            json.dump(data, json_file, indent=4)


if __name__ == "__main__":
    asyncio.run(run())
