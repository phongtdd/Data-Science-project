{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split , cross_val_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\DS\\crwa\\Data-Science-project\\preprocess_data.json', 'r') as file:\n",
    "    json_data = file.read()\n",
    "\n",
    "df = pd.read_json(StringIO(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['url','name','asin']\n",
    "df.drop(columns=col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['department'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_categorical_data(X, category_columns):\n",
    "    \"\"\"\n",
    "    Balances the dataset by duplicating rows from under-represented categories in a specified categorical column.\n",
    "    \n",
    "    Parameters:\n",
    "    X (pd.DataFrame): The feature data with categorical features.\n",
    "    category_column (str): The column name containing the categorical feature to balance.\n",
    "    \n",
    "    Returns:\n",
    "    X_resampled (pd.DataFrame): The resampled feature data.\n",
    "    \"\"\"\n",
    "    for category_column in category_columns:\n",
    "        # Count the occurrences of each category in the specified column\n",
    "        category_counts = X[category_column].value_counts()\n",
    "        \n",
    "        # Identify the category with the most and least occurrences\n",
    "        majority_category = category_counts.idxmax()\n",
    "        minority_category = category_counts.idxmin()\n",
    "        \n",
    "        # Get the number of occurrences of the majority and minority categories\n",
    "        majority_count = category_counts[majority_category]\n",
    "        minority_count = category_counts[minority_category]\n",
    "        \n",
    "        # Find how many rows need to be added for each minority category\n",
    "        duplication_factor = majority_count // (minority_count*3)\n",
    "        \n",
    "        # Separate rows belonging to the minority category\n",
    "        minority_data = X[X[category_column] == minority_category]\n",
    "        \n",
    "        # Duplicate the rows for the minority category\n",
    "        X = pd.concat([X] + [minority_data] * duplication_factor, axis=0)\n",
    "        \n",
    "        # Shuffle the dataset to ensure randomness after duplication\n",
    "        X = X.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance_col = ['department', 'origin', 'Not Bleach', 'Tumble Dry', 'Tie', 'No closure', 'Elastic', 'Lace Up', 'Drawstring']\n",
    "df = balance_categorical_data(df,imbalance_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'price'\n",
    "features = [col for col in df.columns if col != target]\n",
    "x = df[features]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_temp, y_train, y_temp = train_test_split(x,y, test_size=0.2, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV , KFold \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=cv, scoring=['r2','neg_mean_squared_error'], refit='neg_mean_squared_error', verbose=2)\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, median_absolute_error, r2_score\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(x_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "mae = median_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'R²: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_model, \"random_forest_price_prediction_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "max_depths = [params['max_depth'] for params in results['params']]\n",
    "\n",
    "mean_r2_scores = results['mean_test_r2']\n",
    "mean_neg_mse_scores = results['mean_test_neg_mean_squared_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mse_scores = -1 * np.array(mean_neg_mse_scores)\n",
    "mean_rmse_scores = np.sqrt(mean_mse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(max_depths, mean_r2_scores, 'o-', label=\"R² Score\")\n",
    "plt.title('R² Score vs Max Depth')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('R² Score')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(max_depths, mean_rmse_scores, 'o-', color='orange', label=\"RMSE\")\n",
    "plt.title('RMSE vs Max Depth')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
